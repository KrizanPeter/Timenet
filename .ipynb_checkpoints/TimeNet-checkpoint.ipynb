{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\" #4번 DEVICES만 사용하여 다른 메모리를 낭비를 방지\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "#costomized seq2seq cell\n",
    "import copy\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.ops import variable_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#=======================================Hyperparameter \n",
    "learning_rate = 0.001\n",
    "total_epoch = 1000\n",
    "batch_size = 100 \n",
    "dropout = 0.5\n",
    "hidden_size = 200\n",
    "index = 0\n",
    "\n",
    "feed_train = {}\n",
    "feed_val = {}\n",
    "outputs = []\n",
    "targets = []\n",
    "DATAS = []\n",
    "data_size = 50\n",
    "\n",
    "#훈련에 사용할 데이터 이름을 배열로 저장한다. \n",
    "traindatasets = ['Plane', 'GunPoint', 'ArrowHead', 'WordSynonyms', 'ToeSegmentation1', 'FISH', 'ShapeletSim', 'ShapesAll', 'SonyAIBORobotSurface1',\n",
    "             'Lightning7', 'ToeSegmentation2', 'DiatomSizeReduction', 'Ham', 'SonyAIBORobotSurface2', 'TwoLeadECG', 'FacesUCR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customized_rnn_seq2seq(encoder_inputs,\n",
    "                          decoder_inputs,\n",
    "                          cell,\n",
    "                          dtype=dtypes.float32,\n",
    "                          scope=None):\n",
    "    \n",
    "    with variable_scope.variable_scope(scope or \"basic_rnn_seq2seq\"):\n",
    "        enc_cell = copy.deepcopy(cell)\n",
    "        encoder_outputs, enc_state = tf.contrib.rnn.static_rnn(enc_cell, encoder_inputs, dtype=dtype)\n",
    "    return customized_rnn_decoder(encoder_outputs, decoder_inputs, enc_state, cell)\n",
    "\n",
    "def customized_rnn_decoder(encoder_outputs,\n",
    "                decoder_inputs,\n",
    "                initial_state,\n",
    "                cell,\n",
    "                loop_function=None,\n",
    "                scope=None):\n",
    "    \n",
    "    with variable_scope.variable_scope(scope or \"rnn_decoder\"):\n",
    "        state = initial_state\n",
    "        outputs = []\n",
    "        prev = None\n",
    "        for i, inp in enumerate(decoder_inputs):\n",
    "            if loop_function is not None and prev is not None:\n",
    "                with variable_scope.variable_scope(\"loop_function\", reuse=True):\n",
    "                    inp = loop_function(prev, i)\n",
    "            if i > 0:\n",
    "                variable_scope.get_variable_scope().reuse_variables()\n",
    "            output, state = cell(inp, state)\n",
    "            outputs.append(output)\n",
    "            if loop_function is not None:\n",
    "                prev = output\n",
    "    return encoder_outputs, outputs, state\n",
    "\n",
    "def data_maker(dataset):\n",
    "    print(dataset)\n",
    "    #train 숫자가 너무 작아서 concatenate이후 train_test_split를 이용해 다시 나눈다.\n",
    "    datadir = 'UCRArchive_2018' + '/' + dataset + '/' + dataset\n",
    "    data_train = np.loadtxt(datadir+'_TRAIN.tsv', delimiter='\\t')\n",
    "    data_test = np.loadtxt(datadir+'_TEST.tsv', delimiter='\\t')\n",
    "    DATA = np.concatenate((data_train,data_test),axis=0)\n",
    "    \n",
    "    #특정 데이터의 y lable이 1부터 시작해 range error 발생.첫 인덱스가 1이면 1을 빼서 0으로 만듬\n",
    "    X_data = DATA[:,1:]\n",
    "    y_data = DATA[:,[0]]\n",
    "    y_first_index = int(np.unique(y_data)[0])\n",
    "    if int(y_first_index) == 1:\n",
    "        y_data = DATA[:,[0]]-1\n",
    "    DATA = np.concatenate((y_data, X_data), axis=1)\n",
    "    \n",
    "    #50행마다 잘라서 DATAS 배열에 저정\n",
    "    iter = DATA.shape[0]//data_size\n",
    "    for i in range(iter):\n",
    "        CUT_DATA = DATA[50*i:50*(i+1),:]\n",
    "        DATAS.append(CUT_DATA)\n",
    "    \n",
    "def seq2seq_maker(index, DATA):\n",
    "\n",
    "    X_data = DATA[:,1:]\n",
    "    y_data = DATA[:,0]\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.2, random_state=1)\n",
    "    \n",
    "    encoder_input = tf.placeholder(tf.float32, [None, X_train.shape[1]]) \n",
    "    decoder_input = tf.placeholder(tf.float32, [None, X_train.shape[1]])\n",
    "    target = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "    targets.append(target)\n",
    "    \n",
    "    with tf.variable_scope(\"rnn_\"+str(index)):\n",
    "        cell = tf.contrib.rnn.GRUCell(num_units=hidden_size)\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=0.5)\n",
    "        #output, states = tf.contrib.legacy_seq2seq.basic_rnn_seq2seq([encoder_input], [decoder_input], cell)\n",
    "        en_outputs, de_outputs, state = customized_rnn_seq2seq([encoder_input], [decoder_input], cell)\n",
    "        #print(\"encoder_outputs\", encoder_outputs)\n",
    "        #print(\"decoder_outputs\", decoder_outputs)\n",
    "        de_outputs = tf.reshape(de_outputs, [-1, hidden_size]) #3D -> 2D\n",
    "        outputs.append(de_outputs)\n",
    "    \n",
    "    feed_train[encoder_input] = X_train\n",
    "    feed_train[decoder_input] = X_train\n",
    "    feed_train[target] = y_train\n",
    "    \n",
    "    feed_val[encoder_input] = X_val\n",
    "    feed_val[decoder_input] = X_val\n",
    "    feed_val[target] = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plane\n",
      "GunPoint\n",
      "ArrowHead\n",
      "WordSynonyms\n",
      "ToeSegmentation1\n",
      "FISH\n",
      "ShapeletSim\n",
      "ShapesAll\n",
      "SonyAIBORobotSurface1\n",
      "Lightning7\n",
      "ToeSegmentation2\n",
      "DiatomSizeReduction\n",
      "Ham\n",
      "SonyAIBORobotSurface2\n",
      "TwoLeadECG\n",
      "FacesUCR\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-3-85b1fee72b4a>:71: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-3-85b1fee72b4a>:9: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\kriza\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\kriza\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\kriza\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "#data_maker('Plane')        \n",
    "for dataset in traindatasets:\n",
    "    data_maker(dataset)    \n",
    "    \n",
    "for index, DATA in enumerate(DATAS):\n",
    "    seq2seq_maker(index, DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W <tf.Variable 'W:0' shape=(200, 100) dtype=float32_ref>\n",
      "b <tf.Variable 'b:0' shape=(100,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable(tf.random_normal([hidden_size, 100]), name=\"W\") \n",
    "b = tf.Variable(tf.random_normal([100]), name=\"b\") #최대 클래수의 갯수인 60보다는 커야 range 에러 안생김\n",
    "print(\"W\", W)\n",
    "print(\"b\", b)\n",
    "logits = [tf.matmul(output, W) + b for output in outputs]\n",
    "\n",
    "with tf.variable_scope(\"cost\"):\n",
    "    loss = []\n",
    "    for logit, target in zip(logits, targets):\n",
    "        loss.append(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=target))\n",
    "    #cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets))\n",
    "    cost= tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost) \n",
    "    \n",
    "#with tf.variable_scope(\"eval\"):\n",
    "#    prediction = tf.argmax(tf.nn.softmax(logits), 1) \n",
    "#    correct_prediction = tf.equal(prediction, targets)#one-hot을 안쓰면 target에는 argmax할 필요없음\n",
    "#    accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.7752874   2.0469844   0.36769435 ... -0.8887142  -0.39307138\n",
      "  -0.18026935]\n",
      " [ 0.35898998  1.1710458  -0.5539473  ...  1.610424   -0.3653937\n",
      "   0.2267573 ]\n",
      " [ 1.7162745   3.4374948   0.18480867 ... -0.33494365 -0.70481473\n",
      "   1.5762633 ]\n",
      " ...\n",
      " [ 1.0546646   2.1506813  -1.7801001  ... -0.08220769  1.5439498\n",
      "   1.8349516 ]\n",
      " [-2.4351687   1.5379708  -0.27892506 ... -2.2651443   0.2654991\n",
      "   0.12807518]\n",
      " [ 0.6646134   0.41552642 -1.6548251  ... -0.5516775   0.41803804\n",
      "   0.48632395]]\n",
      "[ 1.0229144   0.10713756  1.2602333  -0.18442847  1.0593863   0.5241448\n",
      "  0.38003686  2.0510676   0.34854162 -0.46839184 -1.3516546  -1.2927005\n",
      "  0.2775945   0.45241794  1.7855079   0.1428486   2.197832   -0.05300001\n",
      "  0.56384265 -0.3621135   1.0877128   1.0643741  -1.7113878  -1.8861148\n",
      "  0.49191344 -1.0251831  -1.8322444  -0.19497852 -0.0093414   0.5123816\n",
      " -0.63917947  0.29399693 -0.3947946  -0.03477793 -0.22016911  0.18740991\n",
      " -0.6377595  -0.6909403   1.8086764   0.1957868   1.1917068   0.3121187\n",
      "  0.5106586  -1.4103146  -1.4999449  -2.7044766   0.66161746 -0.10199916\n",
      " -0.02279397  1.2707777   0.8595847  -0.51078475  0.9622572  -0.88360846\n",
      " -0.7098269  -1.4913269  -1.0287327  -0.40101263 -1.0951536  -0.25585675\n",
      " -0.92576957  0.13322021  0.9284341   0.09194557  0.2886534  -1.099663\n",
      " -0.11733882  0.6584703  -0.49212244  1.378654    0.31270203 -0.50857097\n",
      " -1.8607545  -0.6103319  -1.0786191  -1.5422686   0.13954127 -1.4184303\n",
      "  2.0137646  -0.07396756  0.11858455 -1.0129801  -0.0852141  -0.78480124\n",
      " -0.49538723  1.0627272  -0.99274904 -0.14849328  0.6173164   1.992241\n",
      "  0.02163593 -0.27713823 -0.76058984 -0.95922816 -0.055263    1.8076211\n",
      "  1.1680217   1.7386084   0.6248893  -0.45595476]\n",
      "============================sess init\n",
      "Epoch 1/1000 took 22.463s\n",
      "  Train      loss : 11.244384\n",
      "  Validation loss : 7.637080\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    print(sess.run(\"W:0\"))\n",
    "    print(sess.run(\"b:0\"))\n",
    "    print(\"============================sess init\")\n",
    "    start_time = time.time()\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(total_epoch):\n",
    "        _, tr_loss = sess.run([optimizer, cost], feed_dict = feed_train)\n",
    "        _, va_loss = sess.run([optimizer, cost], feed_dict = feed_val)\n",
    "        train_losses += [tr_loss]\n",
    "        valid_losses += [va_loss]\n",
    "        if epoch % 100 == 0:  \n",
    "            print(\"Epoch {}/{} took {:.3f}s\".format(epoch + 1, total_epoch,time.time() - start_time))\n",
    "            print(\"  Train      loss : %.6f\"%(train_losses[epoch]))\n",
    "            print(\"  Validation loss : %.6f\"%(valid_losses[epoch]))\n",
    "    print(\"It took\", time.time() - start_time, \"seconds to train for\", total_epoch, \"epochs.\")        \n",
    "    print(\"============================ training end\")\n",
    "    \n",
    "    #loss  그래프를 확인한다. \n",
    "    plt.plot(train_losses, '-b', label='Train loss')\n",
    "    plt.plot(valid_losses, '-r', label='Valid loss')\n",
    "    plt.legend(loc=0)\n",
    "    plt.title('Loss graph')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    #print('Accuracy:', sess.run(accuracy_op, feed_dict=feed_dict))\n",
    "    \n",
    "    #pred = sess.run(prediction, feed_dict=feed_dict)\n",
    "    #for p, i in zip(pred, y_test.flatten()):\n",
    "    #    print(\"[{}] Prediction: {} True Y: {}\".format(p == int(i), p, int(i)))\n",
    "        \n",
    "    print(sess.run(\"W:0\"))\n",
    "    print(sess.run(\"b:0\"))\n",
    "    save_path = saver.save(sess, \"./180123.ckpt\")\n",
    "    print(\"model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "saver = tf.train.import_meta_graph(\"./180123.ckpt.meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-3-abece4611618>:34: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From C:\\Users\\kriza\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\legacy_seq2seq\\python\\ops\\seq2seq.py:186: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\kriza\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\kriza\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\kriza\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "outputs Tensor(\"seq2seqCoffee/Reshape:0\", shape=(?, 200), dtype=float32)\n",
      "states Tensor(\"seq2seqCoffee/basic_rnn_seq2seq/rnn_decoder/gru_cell/add:0\", shape=(?, 200), dtype=float32)\n",
      "W Tensor(\"W:0\", shape=(200, 100), dtype=float32_ref)\n",
      "b Tensor(\"b:0\", shape=(100,), dtype=float32_ref)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "total_epoch = 1000\n",
    "batch_size = 100 \n",
    "dropout = 0.5\n",
    "hidden_size = 200\n",
    "index = 0\n",
    "\n",
    "dataset = 'Coffee'\n",
    "datadir = 'UCRArchive_2018' + '/' + dataset + '/' + dataset\n",
    "data_train = np.loadtxt(datadir+'_TRAIN.tsv', delimiter='\\t')\n",
    "data_test = np.loadtxt(datadir+'_TEST.tsv', delimiter='\\t')\n",
    "\n",
    "#train 숫자가 너무 작아서 concatenate이후 train_test_split를 이용해 다시 나눈다.\n",
    "DATA = np.concatenate((data_train,data_test),axis=0)\n",
    "X_data_n = DATA[:,1:]\n",
    "y_data_n = DATA[:,0]\n",
    "\n",
    "#특정 데이터의 y lable이 1부터 시작해 range error 발생.첫 인덱스가 1이면 1을 빼서 0으로 만듬\n",
    "y_first_index = int(np.unique(y_data_n)[0])\n",
    "if int(y_first_index) == 1:\n",
    "    y_data_n = DATA[:,0]-1\n",
    "\n",
    "X_train_n, X_test_n, y_train_n, y_test_n = train_test_split(X_data_n, y_data_n, test_size=0.2, random_state=1)\n",
    "X_train_n, X_val_n, y_train_n, y_val_n = train_test_split(X_train_n, y_train_n, test_size=0.2, random_state=1)\n",
    "\n",
    "n_variable = X_train_n.shape[1]\n",
    "\n",
    "# placeholder\n",
    "encoder_inputs = tf.placeholder(tf.float32, [None, n_variable], name=\"encoder_inputs\")\n",
    "decoder_inputs = tf.placeholder(tf.float32, [None, n_variable], name=\"decoder_inputs\")\n",
    "targets = tf.placeholder(tf.int64, [None], name=\"targets\")\n",
    "\n",
    "with tf.variable_scope(\"seq2seq\"+dataset):  \n",
    "    cell = tf.contrib.rnn.GRUCell(num_units=hidden_size)\n",
    "    outputs, states = tf.contrib.legacy_seq2seq.basic_rnn_seq2seq([encoder_inputs], [decoder_inputs], cell)\n",
    "    outputs = tf.reshape(outputs, [-1, hidden_size]) #3D -> 2D\n",
    "    print(\"outputs\", outputs)  # output 모양이 항상 [? , hidden_size]으로 고정됨\n",
    "    print(\"states\", states)\n",
    "    \n",
    "    \n",
    "    W = tf.get_default_graph().get_tensor_by_name(\"W:0\")\n",
    "    b = tf.get_default_graph().get_tensor_by_name(\"b:0\")\n",
    "    print(\"W\", W)\n",
    "    print(\"b\", b)\n",
    "    logits = tf.matmul(outputs, W) + b\n",
    "\n",
    "with tf.variable_scope(\"cost\"+dataset):\n",
    "    cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost) \n",
    "\n",
    "#print(optimizer)\n",
    "    \n",
    "with tf.variable_scope(\"eval\"+dataset):\n",
    "    prediction = tf.argmax(tf.nn.softmax(logits), 1) \n",
    "    correct_prediction = tf.equal(prediction, targets)#one-hot을 안쓰면 target에는 argmax할 필요없음\n",
    "    accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./180123.ckpt\n",
      "... check point loaded\n",
      "[[ 2.2213447   0.3219719   0.490732   ...  0.4942872   0.83331865\n",
      "   2.3464181 ]\n",
      " [ 0.9293427  -0.5835477  -0.60782105 ... -0.7719066   0.09963955\n",
      "   0.48788664]\n",
      " [-0.6694349  -0.43705964  0.8162472  ... -1.2734629   1.355652\n",
      "   0.48305488]\n",
      " ...\n",
      " [-1.0361733   0.13412148  0.30746385 ... -1.0050983   0.26483974\n",
      "  -0.40772647]\n",
      " [ 0.58828133  0.6410695  -1.0061091  ... -0.98361045  0.06794253\n",
      "   0.02531427]\n",
      " [-0.769398   -1.3087723   0.00831522 ... -0.22830899  0.42123502\n",
      "  -0.62423563]]\n",
      "[ 2.6085396  -0.13781853  0.5079586   0.7505565  -1.6711701  -1.1244154\n",
      " -1.2220967  -0.9417593  -0.75413495  0.7835153   0.76824826  0.01421006\n",
      " -0.84674037 -0.36326706 -0.89012754  1.6949654  -0.17451143 -0.17321901\n",
      "  0.25079346  0.96105075 -0.19576448  0.18262634 -0.04339788 -0.44675002\n",
      "  0.03818896 -0.5850081   0.45266193  0.30945715 -0.03430903  0.50403404\n",
      "  0.3875951   0.7881022  -0.9923665   1.1446277   0.35451147 -0.78186536\n",
      "  0.2636655  -0.6810216   0.48509568 -1.1281995   1.0134664  -0.6925074\n",
      " -0.4657965  -0.2842264  -0.19420502 -0.64189523  0.1927023  -1.1930281\n",
      "  0.4399041  -1.1250904   0.82068926  1.9889681   0.5169783   1.7859102\n",
      " -1.7405604  -1.4908129   1.2667551   0.20525137 -1.1473598  -1.1852242\n",
      "  0.6606851   0.7973213  -0.21778503  0.93277043  1.4845728  -1.7786216\n",
      "  0.69635725 -0.6914711  -0.59305555  0.27201623 -1.5651445  -0.3422639\n",
      "  0.69554144 -2.868164   -1.4379076  -0.55696255 -1.6639522   1.8667883\n",
      " -0.978083   -0.29593268 -0.60559607 -0.58895946 -1.2846706   0.32581523\n",
      " -0.21133175 -0.03026143 -0.07387211  0.47515836 -0.30159453  1.1795429\n",
      "  0.42657632 -0.20892018  0.9405378  -0.7922269  -2.6546562   0.20845534\n",
      "  1.7034365  -0.35825625  0.342946    0.23999697]\n",
      "=============================== training Coffee\n",
      "Epoch 1/1000 took 1.819s\n",
      "  Train loss : 4.535911\n",
      "  valid loss : 0.770093\n",
      "Epoch 101/1000 took 3.109s\n",
      "  Train loss : 0.004867\n",
      "  valid loss : 0.001603\n",
      "Epoch 201/1000 took 4.379s\n",
      "  Train loss : 0.001556\n",
      "  valid loss : 0.000423\n",
      "Epoch 301/1000 took 5.641s\n",
      "  Train loss : 0.000553\n",
      "  valid loss : 0.000166\n",
      "Epoch 401/1000 took 6.913s\n",
      "  Train loss : 0.000236\n",
      "  valid loss : 0.000078\n",
      "Epoch 501/1000 took 8.187s\n",
      "  Train loss : 0.000130\n",
      "  valid loss : 0.000051\n",
      "Epoch 601/1000 took 9.450s\n",
      "  Train loss : 0.000108\n",
      "  valid loss : 0.000037\n",
      "Epoch 701/1000 took 10.708s\n",
      "  Train loss : 0.000074\n",
      "  valid loss : 0.000025\n",
      "Epoch 801/1000 took 11.969s\n",
      "  Train loss : 0.000066\n",
      "  valid loss : 0.000017\n",
      "Epoch 901/1000 took 13.229s\n",
      "  Train loss : 0.000053\n",
      "  valid loss : 0.000013\n",
      "============================ training end\n",
      "Accuracy: 1.0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "new_saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver.restore(sess, \"./180123.ckpt\")\n",
    "    print(\"... check point loaded\")\n",
    "    print(sess.run(\"W:0\"))\n",
    "    print(sess.run(\"b:0\"))\n",
    "    print(\"=============================== training\", dataset)\n",
    "    start_time = time.time()\n",
    "    # 비용함수을 학습시킨다. \n",
    "    for epoch in range(total_epoch):\n",
    "        #Minibatch \n",
    "        batch_mask = np.random.choice(len(X_train_n), batch_size)\n",
    "        X_batch = X_train_n[batch_mask]\n",
    "        y_batch = y_train_n[batch_mask]\n",
    "\n",
    "        # Compute the losses, 훈련을 통해 w,b가 계속 조정되고 그로 인해 변하는 정확성이나 비용함수를 측정하는 것\n",
    "        _, tr_loss = sess.run([optimizer, cost], feed_dict={encoder_inputs: X_batch, decoder_inputs: X_batch, targets: y_batch})\n",
    "        _, val_loss = sess.run([optimizer, cost], feed_dict={encoder_inputs: X_val_n, decoder_inputs: X_val_n,targets: y_val_n})\n",
    "        # Log the losses\n",
    "        train_losses += [tr_loss]\n",
    "        val_losses += [val_loss]\n",
    "  \n",
    "        if epoch % 100 == 0:  \n",
    "            print(\"Epoch {}/1000 took {:.3f}s\".format(epoch + 1, time.time() - start_time))\n",
    "            print(\"  Train loss : %.6f\"%(train_losses[epoch]))\n",
    "            print(\"  valid loss : %.6f\"%(val_losses[epoch]))\n",
    "\n",
    "    print(\"============================ training end\")\n",
    "    print('Accuracy:', sess.run(accuracy_op, feed_dict={encoder_inputs: X_test_n, decoder_inputs: X_test_n, targets: y_test_n}))\n",
    "    \n",
    "    pred = sess.run(prediction, feed_dict={encoder_inputs: X_test_n, decoder_inputs: X_test_n})\n",
    "    for p, i in zip(pred, y_test_n.flatten()):\n",
    "        print(\"[{}] Prediction: {} True Y: {}\".format(p == int(i), p, int(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
